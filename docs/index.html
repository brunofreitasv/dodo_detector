

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Dodo’s object detection package &mdash; dodo detector 0.5.4 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Package modules" href="modules.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="#" class="icon icon-home"> dodo detector
          

          
          </a>

          
            
            
              <div class="version">
                0.5.4
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Package modules</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">dodo detector</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="#">Docs</a> &raquo;</li>
        
      <li>Dodo’s object detection package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="dodo-s-object-detection-package">
<h1>Dodo’s object detection package<a class="headerlink" href="#dodo-s-object-detection-package" title="Permalink to this headline">¶</a></h1>
<p>This a Python package I made to make object detection easier. Besides the dependencies listed on <code class="docutils literal notranslate"><span class="pre">setup.py</span></code>, it also depends on the <a class="reference external" href="https://github.com/opencv/opencv_contrib">OpenCV 3 nonfree/contrib packages</a>, which include the SURF <a class="footnote-reference" href="#id7" id="id1">[1]</a> and SIFT <a class="footnote-reference" href="#id8" id="id2">[2]</a> keypoint detection algorithms, as well as the <a class="reference external" href="https://github.com/tensorflow/models/tree/master/research/object_detection">TensorFlow Object Detection API</a>. The documentation over there teaches everything you need to know to install it.</p>
<p>Since this package is not on PyPI, you can install it via <code class="docutils literal notranslate"><span class="pre">pip</span></code> like this:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>pip install git+https://github.com/douglasrizzo/dodo_detector.git
</pre></div>
</div>
<p>TensorFlow is only a soft dependency of the package. If you want GPU support, install the <code class="docutils literal notranslate"><span class="pre">tensorflow-gpu</span></code> package. Otherwise, install <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code>. These soft dependencies can be installed like so:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/douglasrizzo/dodo_detector.git
pip install dodo_detector<span class="o">[</span>tf-cpu<span class="o">]</span> <span class="c1"># for CPU support</span>
pip install dodo_detector<span class="o">[</span>tf-gpu<span class="o">]</span> <span class="c1"># for GPU support</span>
</pre></div>
</div>
<p>OpenCV is a hard dependency and is installed via the PyPI <code class="docutils literal notranslate"><span class="pre">opencv-python</span></code> package. If you already have OpenCV installed (<em>e.g.</em> from source), edit <em>setup.py</em> and remove the hard dependency before installing.</p>
<div class="section" id="quick-start">
<h2>Quick start<a class="headerlink" href="#quick-start" title="Permalink to this headline">¶</a></h2>
<p>The package has two types of detector, a keypoint-based detector and an detector based on pre-trained convolutional neural networks from the TensorFlow <a class="reference external" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md">model zoo</a>.</p>
<p>All detectors have a common interface, with three methods:</p>
<ul>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">from_camera</span></code> takes a camera ID and uses OpenCV to read a frame stream, which is displayed on a separate window;</p>
</li>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">from_video</span></code> receives a video file and also displays the detection results on a window;</p>
</li>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">from_image</span></code> receives a single RGB image as a numpy array and returns a tuple containing an image with all the detected objects marked in it, and a dictionary containing object classes as keys and their bounding boxes in tuples. An example with one apple and two oranges detected in an image:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;apple&#39;</span><span class="p">:</span> <span class="p">[[</span><span class="mi">15</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">200</span><span class="p">,</span><span class="mi">400</span><span class="p">]],</span>
    <span class="s1">&#39;orange&#39;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">[</span><span class="mi">27</span><span class="p">,</span><span class="mi">42</span><span class="p">,</span><span class="mi">215</span><span class="p">,</span><span class="mi">450</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">112</span><span class="p">,</span><span class="mi">117</span><span class="p">,</span><span class="mi">600</span><span class="p">,</span><span class="mi">542</span><span class="p">]</span>
        <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
<div class="section" id="keypoint-based-detector">
<h3>Keypoint-based detector<a class="headerlink" href="#keypoint-based-detector" title="Permalink to this headline">¶</a></h3>
<p>The keypoint-based object detector uses OpenCV 3 keypoint detection and description algorithms, namely, SURF <a class="footnote-reference" href="#id7" id="id3">[1]</a>, SIFT <a class="footnote-reference" href="#id8" id="id4">[2]</a> and RootSIFT <a class="footnote-reference" href="#id9" id="id5">[3]</a>) together with feature matching algorithms in order to detect textures from a database directory on an image. I basically followed <a class="reference external" href="https://docs.opencv.org/3.4.1/d1/de0/tutorial_py_feature_homography.html">this tutorial</a> and implemented it in a more organized way.</p>
<p>Since OpenCV has no implementation of RootSIFT, I stole <a class="reference external" href="https://www.pyimagesearch.com/2015/04/13/implementing-rootsift-in-python-and-opencv/">this one</a>.</p>
<p>Example on running a keypoint-based detector:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dodo_detector.detection</span> <span class="kn">import</span> <span class="n">KeypointObjectDetector</span>
<span class="n">detector</span> <span class="o">=</span> <span class="n">KeypointObjectDetector</span><span class="p">(</span><span class="s1">&#39;/path/to/my/database_dir&#39;</span><span class="p">)</span>
<span class="n">marked_image</span><span class="p">,</span> <span class="n">obj_dict</span> <span class="o">=</span> <span class="n">detector</span><span class="o">.</span><span class="n">from_image</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
</pre></div>
</div>
<p>The database directory must have the following structure:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">database_dir</span>
    <span class="n">beer_can</span>
        <span class="n">img1</span><span class="o">.</span><span class="n">jpg</span>
        <span class="n">img2</span><span class="o">.</span><span class="n">jpg</span>
        <span class="n">img3</span><span class="o">.</span><span class="n">jpg</span>
    <span class="n">milk_box</span>
        <span class="n">hauihu</span><span class="o">.</span><span class="n">jpg</span>
        <span class="mf">172812.</span><span class="n">jpg</span>
        <span class="n">you_require_additional_pylons</span><span class="o">.</span><span class="n">jpg</span>
    <span class="n">chocolate_milk</span>
        <span class="o">.</span>
        <span class="o">.</span>
    <span class="o">.</span>
    <span class="o">.</span>
</pre></div>
</div>
<p>Basically, the top-level directory will contain subdirectories. The name of each subdirectory is the class name the program will return during detection. Inside each subdirectory is a collection of image files, whose keypoints will be extracted by the <code class="docutils literal notranslate"><span class="pre">KeypointObjectDetector</span></code> during the object construction. The keypoints will then be kept in-memory while the object exists.</p>
<p>You can then use the methods provided by the detector to detect objects in your images, videos or camera feed.</p>
</div>
<div class="section" id="convolutional-neural-network-detector-4">
<h3>Convolutional neural network detector <a class="footnote-reference" href="#id10" id="id6">[4]</a><a class="headerlink" href="#convolutional-neural-network-detector-4" title="Permalink to this headline">¶</a></h3>
<p>This detector uses TensorFlow Object Detection API. In order to use it, you must either train your own neural network using their API, or provide a trained network. I have a concise <a class="reference external" href="https://gist.github.com/douglasrizzo/c70e186678f126f1b9005ca83d8bd2ce">tutorial</a> on how to train a neural network, with other useful links.</p>
<p>The resultant training procedure will give you the <em>frozen inference graph</em>, which is a <code class="docutils literal notranslate"><span class="pre">.pb</span></code> file; and a <em>label map</em>, which is a text file with extension <code class="docutils literal notranslate"><span class="pre">.pbtxt</span></code> containing the names of your object classes.</p>
<p>When creating the single-shot detector, the path to the frozen inference graph and label map must be passed. The number of classes can be explicitly passed, or else classes will be counted from the contents of the label map.</p>
<p>Example on running a single-shot detector:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dodo_detector.detection</span> <span class="kn">import</span> <span class="n">SingleShotDetector</span>
<span class="n">detector</span> <span class="o">=</span> <span class="n">SingleShotDetector</span><span class="p">(</span><span class="s1">&#39;path/to/frozen/graph.pb&#39;</span><span class="p">,</span> <span class="s1">&#39;path/to/labels.pbtxt&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">marked_image</span><span class="p">,</span> <span class="n">obj_dict</span> <span class="o">=</span> <span class="n">detector</span><span class="o">.</span><span class="n">from_image</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
</pre></div>
</div>
<p>Have fun!</p>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="id7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><em>(<a class="fn-backref" href="#id1">1</a>, <a class="fn-backref" href="#id3">2</a>)</em> <ol class="last upperalpha simple" start="8">
<li>Bay, A. Ess, T. Tuytelaars, and L. Van Gool, “Speeded-up robust features (SURF),” Computer vision and image understanding, vol. 110, no. 3, pp. 346–359, 2008.</li>
</ol>
</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><em>(<a class="fn-backref" href="#id2">1</a>, <a class="fn-backref" href="#id4">2</a>)</em> <ol class="last upperalpha simple" start="4">
<li><ol class="first upperalpha" start="7">
<li>Lowe, “Object recognition from local scale-invariant features,” in Proceedings of the Seventh IEEE International Conference on Computer Vision, 1999, vol. 2, pp. 1150–1157.</li>
</ol>
</li>
</ol>
</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id5">[3]</a></td><td><ol class="first last upperalpha simple" start="18">
<li>Arandjelović and A. Zisserman, “Three things everyone should know to improve object retrieval,” in 2012 IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 2911–2918.</li>
</ol>
</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id6">[4]</a></td><td><ol class="first last upperalpha simple" start="23">
<li>Liu et al., “SSD: Single Shot MultiBox Detector,” arXiv:1512.02325 [cs], vol. 9905, pp. 21–37, 2016.</li>
</ol>
</td></tr>
</tbody>
</table>
<div class="toctree-wrapper compound">
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Package modules</a><ul>
<li class="toctree-l2"><a class="reference internal" href="dodo_detector.html">dodo_detector.detection module</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="modules.html" class="btn btn-neutral float-right" title="Package modules" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Douglas De Rizzo Meneghetti

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
    

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>